version: '3.8'

services:
  llama-server:
    build: .
    ports:
      - "8082:8082"
    volumes:
      - ./models:/models
    restart: unless-stopped
    command: ["./build/bin/llama-server", "-m", "/models/model.gguf", "--port", "8082", "--host", "0.0.0.0"]

